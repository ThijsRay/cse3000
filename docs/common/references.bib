% Encoding: UTF-8

@article{kohn_2018_gender,
  title="{Gender Stereotype Analysis on Reddit using Word Embeddings}",
  author={Kohn, Aaron},
  year=2018,
  month=03
}

@ARTICLE{bolukbasi_2016_quantifying_stereotypes,
       author = {{Bolukbasi}, Tolga and {Chang}, Kai-Wei and {Zou}, James and
         {Saligrama}, Venkatesh and {Kalai}, Adam},
        title = "{Quantifying and Reducing Stereotypes in Word Embeddings}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2016,
        month = jun,
          eid = {arXiv:1606.06121},
        pages = {arXiv:1606.06121},
archivePrefix = {arXiv},
       eprint = {1606.06121},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160606121B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{caliskan_2017_semantics_language_corpora,
title = "Semantics derived automatically from language corpora contain human-like biases",
abstract = "Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",
keywords = "cs.AI, cs.CL, cs.CY, cs.LG",
author = "Aylin Caliskan and Bryson, {Joanna J} and Arvind Narayanan",
year = "2017",
month = "4",
day = "14",
doi = "10.1126/science.aal4230",
language = "English",
volume = "356",
pages = "183--186",
journal = "Science",
issn = "0036-8075",
publisher = "American Association for the Advancement of Science",
number = "6334",

}

@article{nissim_fair_is_better_2020,
author = {Nissim, Malvina and van Noord, Rik and van der Goot, Rob},
title = {Fair is Better than Sensational: Man is to Doctor as Woman is to Doctor},
journal = {Computational Linguistics},
volume = {0},
number = {ja},
pages = {1-17},
year = {2020},
month = {03},
doi = {10.1162/COLI\_a\_00379},
URL = { https://doi.org/10.1162/COLI_a_00379 },
eprint = { https://doi.org/10.1162/COLI_a_00379 },
abstract = { Analogies such as man is to king as woman is to X are often used to illustrate the amazing power of word embeddings. Concurrently, they have also been used to expose how strongly human biases are encoded in vector spaces trained on natural language, with examples like man is to computer programmer as woman is to homemaker. Recent work has shown that analogies are in fact not an accurate diagnostic for bias, but this does not mean that they are not used anymore, or that their legacy is fading. Instead of focusing on the intrinsic problems of the analogy task as a bias detection tool, we discuss a series of issues involving implementation as well as subjective choices, which might have yielded a distorted picture of bias in word embeddings. We stand by the truth that human biases are present in word embeddings, and of course, to the need to address them. But analogies are not an accurate tool to do so, and the way they have been most often used has exacerbated some possibly non-existing biases and perhaps hid others. Because they are still widely popular, and some of them have become classics within and outside the NLP community, we deem it important to provide a series of clarifications that should put well-known, and potentially new analogies into the right perspective. }
}

@inproceedings{gonen-goldberg-2019-lipstick-pig,
    title = "Lipstick on a Pig: {D}ebiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them",
    author = "Gonen, Hila  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1061",
    doi = "10.18653/v1/N19-1061",
    pages = "609--614",
    abstract = "Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between {``}gender-neutralized{''} words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.",
}

@ARTICLE{2017arXiv171108412G,
       author = {{Garg}, Nikhil and {Schiebinger}, Londa and {Jurafsky}, Dan and
         {Zou}, James},
        title = "{Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
         year = 2017,
        month = nov,
          eid = {arXiv:1711.08412},
        pages = {arXiv:1711.08412},
archivePrefix = {arXiv},
       eprint = {1711.08412},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv171108412G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018arXiv180309288K,
       author = {{Kozlowski}, Austin C. and {Taddy}, Matt and {Evans}, James A.},
        title = "{The Geometry of Culture: Analyzing Meaning through Word Embeddings}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2018,
        month = mar,
          eid = {arXiv:1803.09288},
        pages = {arXiv:1803.09288},
archivePrefix = {arXiv},
       eprint = {1803.09288},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180309288K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{grave2018learning,
  title={Learning Word Vectors for 157 Languages},
  author={Grave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas},
  booktitle={Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)},
  year={2018}
}

@INPROCEEDINGS{Levy14linguisticregularities,
    author = {Omer Levy and Yoav Goldberg},
    title = {Linguistic regularities in sparse and explicit word representations},
    booktitle = {In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL)},
    year = {2014}
}

@ARTICLE{2020arXiv200304036Z,
       author = {{Zhu}, Xunjie and {de Melo}, Gerard},
        title = "{Sentence Analogies: Exploring Linguistic Relationships and Regularities in Sentence Embeddings}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2020,
        month = mar,
          eid = {arXiv:2003.04036},
        pages = {arXiv:2003.04036},
archivePrefix = {arXiv},
       eprint = {2003.04036},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200304036Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{li1992random,
  title={Random texts exhibit Zipf's-law-like word frequency distribution},
  author={Li, Wentian},
  journal={IEEE Transactions on information theory},
  volume={38},
  number={6},
  pages={1842--1845},
  year={1992},
  publisher={IEEE}
}


@book{Zipf-1935,
  address    = {Cambridge, Mass.},
  author     = {Zipf, George},
  publisher  = {M.I.T. Press},
  title      = {The Psychobiology of Language: An Introduction to Dynamic Philology},
  year       = {1935},
  iso_code   = {},
  olac_field = {},
  wals_code  = {}
}
