\documentclass[english, a4paper, 10pt]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\usepackage{csquotes}
\usepackage{mathtools}
\usepackage[style=ieee]{biblatex}
\addbibresource{references.bib}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}

\makeatletter
\usepackage{url}

\makeatother

\usepackage{babel}


\title{Research Plan Week 1--2}
\author{Thijs Raymakers}
\date{2020-04-24}

\newcommand{\namelistlabel}[1]{\mbox{#1}\hfil}
\newenvironment{namelist}[1]{%1
\begin{list}{}
    {
        \let\makelabel\namelistlabel
        \settowidth{\labelwidth}{#1}
        \setlength{\leftmargin}{1.1\labelwidth}
    }
  }{%1
\end{list}}

\begin{document}
\maketitle

\begin{namelist}{xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx}
\item[{\bf Title:}]
    Gender bias between languages
\item[{\bf Author:}]
    Thijs Raymakers (4647610)
\item[{\bf Responsible Professor:}]
    David Tax, Marco Loog
%\item[{\bf (Optionally) Other Supervisor:}]
%	Eva
%\item[{\bf (Required for final version) Examiner:}]
%	Another Professor (\emph{interested, but not involved})
\item[{\bf Peer group members:}]
    Pia Keukeleire, 
    Thomas van Tussenbroek,
    David Happel,
    Dina Chen,
    Katja Schmahl
\end{namelist}


\section*{Background of the research}
Word embeddings are being used to capture the meaning and the semantics of words as a
mathematical vector. They are being used in various applications where structural
analysis of text or language is required. However, these word embeddings encode
human biases~\cite{caliskan_2017_semantics_language_corpora, 2019arXiv190509866N, 2017arXiv171108412G, 2018arXiv180309288K, 2019arXiv190303862G}. One of the biases that is often
measured is the gender bias in an embedding. This is classically demonstrated with the
use of analogies such as \textit{man is to king as woman is to queen} or \textit{man is to doctor as woman is to nurse}~\cite{2019arXiv190509866N}.
However, as described in~\cite{2019arXiv190509866N}, one should be careful when using
analogies to measure bias in word embeddings. They describe that the code to calculate
the analogies might be too constrained, since the situation \textit{A is to B as C is to D}
where \textit{D = B} could be impossible to obtain~\cite{2019arXiv190509866N}. Besides
that, it would not always be clear what a factual answer of \textit{D} would be.

This raises an interesting question on how to measure bias in word embeddings without
having the problems that were addressed in~\cite{2019arXiv190509866N}.
In this research, I will not address this issue, but something that is closely related.

\section*{Research Question}
The research would focus on the following question:

\begin{quote}
    To what extend differ word embeddings trained on different languages when comparing
    the words \textit{man} and \textit{woman}?
\end{quote}

My hypothesis here would be that the difference between word embeddings
trained on languages in the same branch of language families is relatively small, while
the difference between word embeddings trained on languages in different branches of
language families is relatively large.
For example, I expect that the differences between Dutch and English are
relatively small, since they are both Germanic languages. On the other hand, I expect the
difference between English and Chinese to be relatively large, since they are not in the
same branch of language families.

\section*{Method}
I will use the word vectors provided by~\cite{grave2018learning}. They provide
pre-trained word vectors for 157 languages, trained using fastText.
Since~\cite{2019arXiv190509866N} addresses some important issues with the classical method
of measuring gender bias, I have to adjust my approach accordingly.
In order to calculate the analogies, I will use the \texttt{3CosMul}
method~\cite{Levy14linguisticregularities}. Based on~\cite{2019arXiv190509866N},
I will most likely use both the constrained and unconstrained version of \texttt{3CosMul}.
While the unconstrained version is significantly less accurate, it performs better is the
well-known \textit{man:doctor :: woman:X} test~\cite{2019arXiv190509866N}.
But the lack of accuracy of the unconstrained version of \texttt{3CosMul} might be
punished when performing other tests, which is why I should also look at the constrained
version of \texttt{3CosMul}.

In order to address the subjective factors described by~\cite{2019arXiv190509866N}, I
will calculate the gender difference of a word embedding $W$ by measuring the ratio
$$
\frac{ |\forall x (A : x :: B :: y) \mid \{x \in W \mid x = y\}| } { |W| }
$$
where $A$ refers to the translation of \textit{man} or \textit{woman} and B refers to the translation of \textit{woman} or \textit{man} respectively.
A ratio of 1 would indicate that $A = B$ and a ratio of 0 would indicate that $A \neq B$.
Values in between would show the similarity between \textit{man} and \textit{woman} in relation to other words in the word embedding.

It is important to note that this does not measure gender bias, but rather the difference between the words in relation to the other words in the word embedding.
While the analogy \textit{man : doctor :: woman : nurse} might occur, the analogy \textit{man:king :: woman:queen} could also occur.
The former might indicate gender bias, the latter does not.
Choosing the words that might indicate gender bias is a subjective task in itself and would probably result in a non-exhaustive set of words that could indicate this.

\section*{Planning of the first two weeks}
The first two weeks of the research project will be filled with a more detailed research
into the exact methods that will be used and the start of a proof of concept.
The following two weeks will also be used to talk with peer group members that will also work with word embeddings, to discuss our thoughts.

After these first two weeks, the research question and the research proposal, will be
more detailed and will provide more details about my approach.

\printbibliography

\end{document}
