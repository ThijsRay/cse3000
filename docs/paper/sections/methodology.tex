\section{Methodology} \label{ch:method}
The word embeddings provided by previous research are used for this
study~\parencite{grave2018learning}.
These word embeddings have been
trained for various different languages on texts from
Wikipedia\footnote{\url{https://wikipedia.org/}} and
the Common Crawl Project\footnote{\url{https://commoncrawl.org/}}.
The languages that are used in this research, are chosen based on the
following criteria;
\begin{enumerate*}[label={(\alph*)}]
    \item the language has a pre-trained word embedding from \textcite{grave2018learning}
    \item the language should be machine translatable with the help of tools like
        DeepL\footnote{\url{https://deepl.com/}} or
        Google Translate\footnote{\url{https://translate.google.com/}} and
    \item the language is part of a different language family than the languages that
    were already chosen.
\end{enumerate*}
This resulted in 26 languages from 16 different language families. An overview of these languages, their language families and the translations of \textit{male} and \textit{female}
can be found in table~\ref{table:languages} in the Appendix.

\subsection{Explanation of WEAT} \label{section:method:weat}
A popular way of measuring bias in word embeddings uses the 
Word Embedding Association Test (WEAT) method by
\textcite{caliskan_2017_semantics_language_corpora}. 
WEAT has been successfully used in previous research~\parencite{gonen-goldberg-2019-lipstick-pig, 10.1145/3306618.3314270}.
This method measures the
association between a set of \textit{attribute words} (e.g. \textit{man}, \textit{male},
\textit{woman}, \textit{female}) 
and a set of \textit{target} words (e.g. \textit{programmer}, \textit{family},
\textit{nurse}, \textit{engineer}). 

The WEAT method calculates the association between attribute words and target words
by measuring the difference in cosine similarity. Formally, 
let $X$ and $Y$ be two equally-sized sets of target words and let $A$ and $B$ be two
sets of attribute words. The test statistic is defined as
\begin{align} \label{eq:weat_s_sum}
    \textstyle s(X,Y,A,B) &= \displaystyle\sum_{\vec{x} \in X} s(\vec{x},A,B)\nonumber \\
                          &- \displaystyle\sum_{\vec{y} \in Y} s(\vec{y},A,B)\quad\text{where}\\
\label{eq:weat_s_mean}
    s(\vec{w},A,B) &= \text{mean}_{\vec{a} \in A} \text{cos}(\vec{w}, \vec{a})
            \nonumber\\
                   &- \text{mean}_{\vec{b} \in B} \text{cos}(\vec{w}, \vec{b})
\end{align}
where $\text{cos}(\overrightarrow{a}, \overrightarrow{b})$ measures the cosine of the angle
between $\vec{a}$ and $\vec{b}$ and where the mean is the sample mean of the values.
Formula~\eqref{eq:weat_s_mean} can be described as a measurement of the association of the
target word $w$ with
an attribute, and formula~\eqref{eq:weat_s_sum} can be described as a measurement of the
differential association of the sets of target words with the attributes~ 
\parencite{caliskan_2017_semantics_language_corpora}.

In WEAT, the effect size $d_1$ of the test statistic is defined as the normalized measure of separation
between two distributions of associations~\parencite{caliskan_2017_semantics_language_corpora}
\begin{align} \label{eq:weat_effect_size}
d_1 = \frac{
    \text{mean}_{\vec{x} \in X} s(\vec{x}, A, B) - \text{mean}_{\vec{y} \in Y} s(\vec{y}, A, B)
}{
\text{std-dev}_{\vec{w} \in X \cup Y} s(\vec{w},A,B)
}
\end{align}
The effect size $d_1$ can be interpreted as the ``amount of bias'' \parencite{lauscher-glavas-2019-consistently}.

\subsection{Weaknesses of WEAT for multilingual use}
The WEAT method works for when measurements are being performed in a single language, but
some problems could arise when the differences between different languages are
compared.

First, not all words have a one-to-one translation into another
language, because certain words might have a different meaning, depending on the
context in which the word is used. It is therefore not possible to translate all the
attribute and target words into a different language, because it might lead to an
incorrect and unfair comparison. A great example of this would be the English word
\textit{man}, which can mean \textit{male} or \textit{mankind}, depending on the context.

Second, the inclusion or exclusion of words from
the target set is a subjective decision~\parencite{nissim_fair_is_better_2020}. Therefore,
the measurement itself could be biased, depending on what words are or are not in the set
of target words. This problem will only be amplified when WEAT is used for multiple
languages, because this subjective decision has to be made for every single language.

\subsection{Proposed multilingual WEAT}
The first problem of translation can be addressed by limiting the amount of words that
have to be
translated. The method that is used in this research only uses the two attribute words 
\textit{male} and \textit{female}. The assumption is made that these words are universal
and translatable in all languages. 

It is proposed to solve the second problem of subjectivity with two different approaches.
The goal is to
remove the subjective decision and include words based on a language-independent metric
that is not based on the meaning of words. This has been done by
\begin{enumerate*}[label={(\arabic*)}]
    \item looking at \emph{all} the words in the word embedding and by
    \item looking at the \emph{most used} words in the word embedding
\end{enumerate*}.
%These approaches will be explained in more detail in section~\ref{method:1} and
%\ref{method:2} respectively.

\subsubsection{Method 1: Uniform weighting}
\label{method:1}
The first method, the uniform weighting method, uses all the words in the word embedding as target words. 
This method uses an adjusted version of the WEAT method described in
section~\ref{section:method:weat}. The adjustments made take into account that
the compared languages have different target and attribute words.
Formulas~\eqref{eq:weat_s_sum} and
\eqref{eq:weat_s_mean} are redefined as 
\begin{align} \label{eq:uniform_s_sum}
    s(X,Y,a,b) &= \displaystyle\sum_{\vec{x} \in X} s(\vec{x},\vec{a_X},\vec{b_X})
    \nonumber \\
               &- \displaystyle\sum_{\vec{y} \in Y} s(\vec{y},\vec{a_Y},\vec{b_Y})
    \quad\text{where}\\
\label{eq:uniform_s_mean}
    s(\vec{w},\vec{a},\vec{b}) &= \text{cos}(\vec{w}, \vec{a}) - \text{cos}(\vec{w}, \vec{b})
\end{align}
where $\text{cos}(\vec{a}, \vec{b})$ measures the cosine distance between between
$\vec{a}$ and $\vec{b}$. 
In formula~\eqref{eq:uniform_s_sum}, $X$ and $Y$ refer to the two languages that are
compared, and $a_X$ and $b_X$ refer to the translations of the
two attribute words $a$ and $b$ into language $X$ respectively. 
The effect size $d_1$ is calculated the same way as WEAT, with
formula~\eqref{eq:weat_effect_size}.

The advantage of this approach is
that all words are considered, which eliminates the inclusion or exclusion subjectiveness
described by \textcite{nissim_fair_is_better_2020}.  This is because there is no metric
on which a word could be excluded, because all words in a language are included by default.
The disadvantage is that all words have the same weight, which might not be an accurate
representation of the language, as not all words are used as often as others. This is
addressed in an alternative method described in section~\ref{method:2}.

\subsubsection{Method 2: Frequency weighting}
\label{method:2}
The disadvantage of method 1, the uniform weighting method, is addressed in method 2,
the frequency weighting method.
The second method still uses all the words in the word embedding, but attaches a
weight to
each word, depending on its usage frequency. Effectively, this results in a measurement
over the most commonly used words, because those words have the highest weight.
%The advantages and disadvantages of this method are discussion in section~\ref{section:discussion_frequency_method}.
The frequency weighting method uses an adjusted version of the uniform weighting method.
The adjustments aim to include information about the usage frequency.
Formulas~\eqref{eq:uniform_s_sum} and~\eqref{eq:uniform_s_mean} are redefined
as
\begin{align} \label{eq:frequency_s_sum}
    s(X,Y,a,b) =& \displaystyle\sum_{\vec{x} \in X}\! f(\vec{x})*s(\vec{x},\vec{a_X},\vec{b_X})
    \nonumber \\
    -& \displaystyle\sum_{\vec{y} \in Y}\! f(\vec{y})\! *\! s(\vec{y},\vec{a_Y},\vec{b_Y})
    \quad\text{where} \\
\label{eq:frequency_s_mean}
s(\vec{w},\vec{a},\vec{b}) =&\ \text{cos}(\vec{w}, \vec{a}) - \text{cos}(\vec{w}, \vec{b})
\end{align}
where $\text{cos}(\vec{a}, \vec{b})$ measures the cosine distance between between
$\vec{a}$ and $\vec{b}$. 
In formula~\eqref{eq:frequency_s_sum}, $X$ and $Y$ refer to the two languages that are
compared, and $a_X$ and $b_X$ refer to the translations of the
two attribute words $a$ and $b$ into language $X$ respectively. 
$f(\vec{x})$ refers to the probability that the word $\vec{x}$ occurs in a text written
in language $X$.

The effect size $d_2$ of the test is defined as an adjusted version of
formula~\eqref{eq:weat_effect_size} that takes the frequency of the words into account:
\begin{equation}\label{eq:frequency_effect_size}
d_2\! =\! \frac{
    \displaystyle\sum_{\vec{x} \in X}\! f(\vec{x})\! *\! s(\vec{x}, A, B)\! -\! \sum_{\vec{y} \in Y}\! f(\vec{y}) * s(\vec{y}, A, B)
}{
\text{std-dev}_{\vec{w} \in X \cup Y} s(\vec{w},A,B)
}
\end{equation}
where the standard deviation takes the frequencies defined by $f$ into account.


%\subsubsection{Test statistics}
%Both of these approaches have been explored. Based on the considerations made above, the
%test statistic used by
%\textcite{caliskan_2017_semantics_language_corpora} can be reused with some adjustments. 
%$$
%\begin{aligned}
%u(w,W,A,B) = \text{mean}_{\forall a \in (A \cap W)}\text{cos}(\overrightarrow{w}, \overrightarrow{a}) \\
%- \text{mean}_{\forall b \in (B \cap W)}\text{cos}(\overrightarrow{w}, \overrightarrow{b})
%\end{aligned}
%$$
%measures the difference between the \textit{cosine similarity} of a word $w$ of language $W$ and the set of attribute words $A$, and of the cosine similarity of a word $w$ and the
%set of attribute words $B$. The $A \cap W$ and $B \cap W$ ensure that the cosine
%similarity is only measured between two vectors in the same word embedding. The
%calculation of this difference can be used in
%$$
%t(w,W,A,B) = Z(\text{rank}(w), |W|) * u(w,W,A,B)\quad\text{where}
%$$
%$$
%Z(k,N) =
%\begin{cases}
%    1 & \quad \text{for method 1} \\
%    \frac{1/k}{\displaystyle\sum_{n=1}^{N}(1/n)} & \quad \text{for method 2}
%\end{cases}
%$$
%This formula adds a weight to each word, depending on its frequency.
%In case the first approach is used, where all words are considered, $Z(k,N)$ will have a
%fixed value of 1 because the first approach does not include frequency.
%In case the second approach is used, the cosine similarity will get a weight attached
%to it because it built upon the assumption that
%if the word \textit{x} occurs more often than the word \textit{y}, then the former should
%be given more weight in the calculation, because that would more accurately reflect the
%usage of the words. The pre-trained models provided by \textcite{grave2018learning} 
%are sorted by frequency, but exact information about how often a
%word occurs is omitted.
%Fortunately, the information about the frequency can be roughly approximated with the
%help of Zipf's law~\parencite{Zipf-1935, word_embedding_zipf_context} or $Z(k,N)$ where
%$k$ is the frequency rank of the word and where $N$ is the number of words in the language,
%or in this case, the word embedding.
%
%This results in a test statistic of
%$$
%s(X,Y,A,B) = \displaystyle\sum_{x \in X}t(x,X,A,B) - \displaystyle\sum_{y \in Y}t(y,Y,A,B)
%$$
%where \textit{A} and \textit{B} are two sets of attribute words, in this case the
%translated versions of the words \textit{male} and
%\textit{female} and where \textit{X} and \textit{Y} are the sets of target words, i.e.
%all words in the word embedding.
%
%The significance of this test statistic is calculated with a \emph{p} value that is
%calculated with an approximated permutation test and is described as
%$$
%\begin{cases}
%    \text{Pr}_i[s(X_i, Y,_i, A, B) > s(X,Y,A,B)] \quad \text{if } s(X,Y,A,B) > 0 \\
%    \text{Pr}_i[s(X_i, Y,_i, A, B) < s(X,Y,A,B)] \quad \text{if } s(X,Y,A,B) < 0
%\end{cases}
%$$
%where $\{(X_i, Y_i)\}_i$ denotes all partitions of $X \cup Y$ into two sets of equal 
%size. 
%
%The effect size $d$ can be described as
%$$
%\frac{
%    \text{mean}_{x \in X} u(x, X, A, B) - \text{mean}_{y \in Y} u(y, Y, A, B)
%}{
%    \text{std-dev}_{w \in X \cup Y} u(w,W,A,B)
%}
%$$
%where $W$ is the language that $w$ is a part of. The effect size $d$ can be interpreted as
%the amount of bias~\parencite{lauscher-glavas-2019-consistently}.
%
%In order investigate whether the null hypothesis $H_0$ can be rejected, these tests have
%been performed with pairs of two languages $X$ and $Y$, where $X$ is an actual word
%embedding
%from languages in table~\ref{table:languages} and where $Y$ is an equally sized dummy
%language that
%contains no bias at all. Language $Y$ can be seen as a language where
%$\forall y \in Y(u(y,Y,A,B) = 0)$.
