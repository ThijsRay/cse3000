\section{Methodology}
This study uses the word embeddings provided by \textcite{grave2018learning}. They have
trained word embeddings for various different languages on texts from Wikipedia and
the Common Crawl Project.

The languages that have been used in this research, have been chosen based on a
few criteria,
\begin{enumerate*}
    \item the language has a pre-trained word embedding from \textcite{grave2018learning}
    \item the language should be machine translatable with the help of tools like DeepL or
    Google Translate and
    \item the language is part of a different language family than the languages that
    were already chosen.
\end{enumerate*}
This resulted 26 languages from 16 different language families. An overview of these languages, their language families and the translations of \textit{male} and \textit{female}
can be found in table~\ref{table:languages}.

The main calculations of this study are focussed on the measurement of gender bias.
A common way of measuring bias in word embeddings uses the 
Word Embedding Association Test (WEAT) method by
\textcite{caliskan_2017_semantics_language_corpora}. This method measure the
association between a set of \textit{attribute words} (e.g. \textit{man}, \textit{woman}) 
and a set of \textit{target} words (e.g. \textit{programmer}, \textit{family},
\textit{nurse}, \textit{engineer}).
WEAT has been used before by \textcite{gonen-goldberg-2019-lipstick-pig}
and \textcite{10.1145/3306618.3314270}.
This approach can work when looking at a single language, but has some problems when
trying to compare differences between different languages.

First, not all words have a one-to-one translation into another
language, because certain words might have a different meaning, depending on the
context in which the word is used. It is therefore not possible to translate all the
attribute and target words into a different language, because it might lead to an
incorrect and unfair comparison. A great example of this would the English word
\textit{man}, which can mean \textit{male} or \textit{mankind}, depending on the context.
Second, the inclusion or exclusion of words from
the target set is a subjective decision~\parencite{nissim_fair_is_better_2020}. Therefore,
the measurement itself could be biased, depending on what words are or are not in the set
of target words. This problem is only amplified if WEAT is used for multiple languages,
because this subjective decision has to be made for every single language.

The first problem can be addressed by limiting the amount of words that have to be
translated. The method that is used in this research uses the two attribute words 
\textit{male} and \textit{female}. The assumption is made that these words are universal
and translatable in all languages. However, this is an assumption that is difficult to
verify without having a good understanding of all the languages that are involved.

The second problem can be addressed in two ways. The goal is to create a reproducible
set of target words for all languages. This can be done by either looking at all words
in the
word embedding, or by looking at the most used words in the word embedding.

\subsection{Method 1}
\label{method:1}
The first way uses
all the words in the word embedding as target words. The advantage of this approach is
that all words are considered, which eliminates the inclusion or exclusion subjectiveness
described by \textcite{nissim_fair_is_better_2020}.  This is because there is no metric
on which a word could be excluded, because all words are included by default.
A disadvantage of this approach
are that the results are difficult to interpret, because words with an explicit
gender (e.g. \textit{he}, \textit{she}, \textit{waiter}, \textit{waitress},
\textit{king}, \textit{queen}) are also
present in the set of target words. This is a problem because these words tend to be biased
towards a gender, but with a valid reason. These words increase the probability of a
type I error, where the null hypothesis is rejected even if the word embedding is not
biased.

\subsection{Method 2}
\label{method:2}
The second way still uses all the words in the word embedding, but attaches a weight to
each word, depending on its frequency. Effectively, this results in a measurement over
the most commonly used words, because those words have the highest weight.
This approach builds upon the assumption that words that are regarded as neutral words
(e.g. \textit{and}, \textit{the}) should be gender neutral, unless there exists some form
of gender bias in the word embedding.

The advantage of this second approach is that results are more representative of how
the language is actually used. While the amount of words associated with men and the
amount of words associated with women might be equal, i.e. each male word has a female
counterpart, they are not used as often.
Taking the frequency of words into account addresses this issue. 
The disadvantage of this is that
it might miss forms of gender bias that might be present in the word embedding, because
not all words are considered to be as important as other words. For example, if the words
\textit{doctor} and \textit{nurse} have a large amount of bias associated with them,
but they only have a weight of $0.00001$, then their reported bias would be almost
invisible when compared to a presumably neutral word like \textit{and}, which might have
a weight of $0.1$. Words that are not used as often will therefore have a negligible impact
on the measurement.

\subsection{Test statistics}
Both of these approaches have been explored. Based on the considerations made above, the
test statistic used by
\textcite{caliskan_2017_semantics_language_corpora} can be reused with some adjustments. 
$$
\begin{aligned}
u(w,W,A,B) = \text{mean}_{\forall a \in (A \cap W)}\text{cos}(\overrightarrow{w}, \overrightarrow{a}) \\
- \text{mean}_{\forall b \in (B \cap W)}\text{cos}(\overrightarrow{w}, \overrightarrow{b})
\end{aligned}
$$
measures the difference between the \textit{cosine similarity} of a word $w$ of language $W$ and the set of attribute words $A$, and of the cosine similarity of a word $w$ and the
set of attribute words $B$. The $A \cap W$ and $B \cap W$ ensure that the cosine
similarity is only measured between two vectors in the same word embedding. The
calculation of this difference can be used in
$$
t(w,W,A,B) = Z(\text{rank}(w), |W|) * u(w,W,A,B)\quad\text{where}
$$
$$
Z(k,N) =
\begin{cases}
    1 & \quad \text{for method 1} \\
    \frac{1/k}{\displaystyle\sum_{n=1}^{N}(1/n)} & \quad \text{for method 2}
\end{cases}
$$
This formula adds a weight to each word, depending on its frequency.
In case the first approach is used, where all words are considered, $Z(k,N)$ will have a
fixed value of 1 because the first approach does not include frequency.
In case the second approach is used, the cosine similarity will get a weight attached
to it because it built upon the assumption that
if the word \textit{x} occurs more often than the word \textit{y}, then the former should
be given more weight in the calculation, because that would more accurately reflect the
usage of the words. The pre-trained models provided by \textcite{grave2018learning} 
are sorted by frequency, but exact information about how often a
word occurs is omitted.
Fortunately, the information about the frequency can be roughly approximated with the
help of Zipf's law~\parencite{Zipf-1935, word_embedding_zipf_context} or $Z(k,N)$ where
$k$ is the frequency rank of the word and where $N$ is the number of words in the language,
or in this case, the word embedding.

This results in a test statistic of
$$
s(X,Y,A,B) = \displaystyle\sum_{x \in X}t(x,X,A,B) - \displaystyle\sum_{y \in Y}t(y,Y,A,B)
$$
where \textit{A} and \textit{B} are two sets of attribute words, in this case the
translated versions of the words \textit{male} and
\textit{female} and where \textit{X} and \textit{Y} are the sets of target words, i.e.
all words in the word embedding.

The significance of this test statistic is calculated with a \emph{p} value that is
calculated with an approximated permutation test and is described as
$$
\begin{cases}
    \text{Pr}_i[s(X_i, Y,_i, A, B) > s(X,Y,A,B)] \quad \text{if } s(X,Y,A,B) > 0 \\
    \text{Pr}_i[s(X_i, Y,_i, A, B) < s(X,Y,A,B)] \quad \text{if } s(X,Y,A,B) < 0
\end{cases}
$$
where $\{(X_i, Y_i)\}_i$ denotes all partitions of $X \cup Y$ into two sets of equal 
size. 

The effect size can be described as
$$
\frac{
    \text{mean}_{x \in X} u(x, X, A, B) - \text{mean}_{y \in Y} u(y, Y, A, B)
}{
    \text{std-dev}_{w \in X \cup Y} u(w,W,A,B)
}
$$
where $W$ is the language that $w$ is a part of. The effect size can be interpreted as
the amount of bias~\parencite{lauscher-glavas-2019-consistently}.

In order investigate whether the null hypothesis $H_0$ can be rejected, these tests have
been performed with pairs of two languages $X$ and $Y$, where $X$ is an actual word
embedding
from languages in table~\ref{table:languages} and where $Y$ is an equally sized dummy
language that
contains no bias at all. Language $Y$ can be seen as a language where
$\forall y \in Y(u(y,Y,A,B) = 0)$.

\iffalse
\begin{table*}
    \centering
    \begin{threeparttable}
        \caption{Chosen languages and their translations}
        \label{table:languages}
        \begin{tabular}{lllll}
            \hline
            Language & Language family & Branch & Translation \emph{male} & Translation \emph{female} \\ \hline
            Arabic & Afroasiatic & Semitic & \arab{الذكر} & \arab{أنثى} \\
            Basque & Isolate & Basque & gizonezkoa & emakumezkoak \\
            Burmese & Sino-Tibetan & Lolo-Burmese & \myanmar{အထီး} & \myanmar{အမျိုးသမီး} \\
            Chinese & Sino-Tibetan & Sinitic & \cjk{男} & \cjk{女}\\
            Dutch & Indo-European & Germanic & mannelijk & vrouwlijk \\
            English & Indo-European & Germanic & male & female \\
            Finnish & Uralic & Finnic & uros & nainen \\
            French & Indo-European & Romance & mâle & femelle \\
            German & Indo-European & Germanic & männlich & weiblich \\
            Greek & Indo-European & Hellenic & \noto{αρσενικός} & \noto{θηλυκός} \\
            Hindi & Indo-European & Indo-Aryan & \hindi{पुरुष} & \hindi{महिला} \\
            Hungarian & Uralic & Ugric & \noto{férfi} & \noto{nő} \\
            Italian & Indo-European & Romance & maschio & femmina \\
            Japanese & Japonic & Japanese & \cjk{男性} & \cjk{女性}\\
            Javanese & Austronesian & Malayo-Polynesian & lanang & wadon \\
            Khmer & Austroasiatic & Khmer & \khmer{បុរស} & \khmer{ស្រី} \\
            Korean & Koreanic & Koreanic & \cjk{남성} & \cjk{여성} \\
            Polish & Indo-European & Balto-Slavic & \noto{mężczyzna} & \noto{kobieta} \\
            Portuguese & Indo-European & Romance & masculino & feminino \\
            Russian & Indo-European & Balto-Slavic & \noto{мужчина} & \noto{женщина} \\
            Spanish & Indo-European & Romance & hombre & mujer \\
            Swedish & Indo-European & Germanic & manlig & kvinnlig \\
            Telugu & Dravidian & South-Central & \telugu{పురుషుడు} & \telugu{స్త్రీ} \\
            Thai & Kra-Dai & Tai & \thai{ชาย} & \thai{หญิง} \\
            Turkish & Turkic & Oghuz & \noto{erkek} & \noto{kadın} \\
            Yoruba & Niger-Congo & Volta-Niger & \noto{akọ} & \noto{abo} \\
            \hline
        \end{tabular}
        \begin{tablenotes}[para,flushleft]
            {\small \textit{Note:} An overview of all the languages that were used in this
            research with their respective translations of \emph{male} and \emph{female}.}
        \end{tablenotes}
    \end{threeparttable}
\end{table*}
\fi
