\section{Introduction}\label{ch:introduction}
Word embeddings have become a useful tool in the field of natural language
processing. They can be used to represent the semantics and the meaning of
words as a vector of numbers, which is used for all sorts of applications, such as
recommendation systems \parencite{10.1145/3219819.3219885} or résumé
parsing \parencite{large-scale-hoang-2017, nasser2018convolutional}. Word
embeddings can be used to solve analogies. An example of this would be that the
vector that represents the word
$\overrightarrow{queen}$ would be equal to $\overrightarrow{king} -
\overrightarrow{man} +
\overrightarrow{woman}$. This analogy would capture an `opposite gender' relation.
Other examples of these analogies would be $\overrightarrow{Paris} -
\overrightarrow{France} + \overrightarrow{Germany} \approx \overrightarrow{Berlin}$,
capturing a `capital of' relation, and
$\overrightarrow{cars} - \overrightarrow{car} + \overrightarrow{apple} \approx
\overrightarrow{apples}$, which would capture a `pluralisation' relation \parencite{vylomova-etal-2016-take}.

However, these word embeddings should be used with care because they have been
shown to contain biases~\parencite{caliskan_2017_semantics_language_corpora}.
\textcite{caliskan_2017_semantics_language_corpora} showed
that words related to science are more related to male terms and that
words related to arts are more related to female terms. This is a form of gender bias
that can be problematic when the word embedding is used in a context where gender bias
should be avoided, such as automatic résumé recommendation. A recent example of a failure
of such a system is Amazons recruiting tool that penalized female candidates, because they
included words related to women on their résumé~\parencite{dastin_2018}.

Recently, efforts have been made to reduce the effect of bias in word embeddings.
Research showed that debiasing word embeddings
is possible to some extent~\parencite{bolukbasi_2016_quantifying_stereotypes}, but another
study showed that this approach may hide the
gender bias, instead of removing it~\parencite{gonen-goldberg-2019-lipstick-pig}.

This paper aims to take a step back and look into whether there is a difference in gender
bias between word embeddings of different languages. Some existing related research has
studied this difference before, looking at
gender bias on Wikipedia across six different languages~\parencite{2015arXiv150106307W},
and looking at biases when different languages, models and training
sources are used~\parencite{lauscher-glavas-2019-consistently}.

Previous efforts have generally been focused on word embeddings of the English language.
However, it might be the
case that some languages are inherently less biased.
This insight could be useful in the search towards a novel
debiasing algorithm. 
Furthermore, this could explain why debiasing is easier or harder for certain languages.

The aim of this research is to investigate possible differences in gender bias in word
embeddings of different languages. This difference has been researched by studying to
what extent word
embeddings of different languages are biased towards gender.
This research question will be answered with the help of the WEAT method by
\textcite{caliskan_2017_semantics_language_corpora}.
