\section{Discussion}
A few observations can be made from figure~\ref{fig:effect_size}. Most
of the languages have an effect size above 0, indicating that most of the reviewed
languages are more associated with \emph{male}. Assuming that there is a relation between
the association of a word and bias, then this could mean that
most of the tested languages have a bias towards \emph{male}. Based on this observation,
the null hypothesis can be rejected.

Another observation is that some languages that are from the same language family have
a similar rank. For example, both Finnish and Hungarian are Uralic languages and have a
relatively high effect size compared to the other languages. A similar pattern can be seen
with Portuguese and Spanish, both Iberian languages. However, this might be a incidental
because some closely related languages seem to have opposite effects, like German and
Dutch. Further research could show whether this effect is indeed incidental or if
there is a underlying reason behind it.

Similar observations can be made from figure~\ref{fig:weffect_size}. Most
of the languages have an effect size above 0, indicating that most used words of the
reviewed languages are more associated with \emph{male} than with \emph{female}. This
is interesting because the most used words of a language say something about how a
language is used by people.
From this result it can be concluded that people are more inclined to use
\mbox{\emph{male}-related words}.

Some of the languages seem to be closer associated with \emph{female}-related words, like
Basque, Hindi and French in figure~\ref{fig:effect_size} and Greek in
figure~\ref{fig:weffect_size}. Why do the words in these languages have a stronger
relation with \emph{female}-related words?
Understanding why this is
the case could be beneficial in the search towards a debiasing algorithm for word
embeddings and possible explanations of why these languages show these characteristics can
be explored in future research.

\subsection{Limitations}

\subsubsection{Uniform weighting method} \label{section:discussion_uniform_method}
The advantage of this approach is
that all words are considered, which eliminates the inclusion or exclusion subjectiveness
described by \textcite{nissim_fair_is_better_2020}.  This is because there is no metric
on which a word could be excluded, because all words are included by default.
A disadvantage of this approach
are that the results are difficult to interpret, because words with an explicit
gender (e.g. \textit{he}, \textit{she}, \textit{waiter}, \textit{waitress},
\textit{king}, \textit{queen}) are also
present in the set of target words. This is a problem because these words tend to be biased
towards a gender, but with a valid reason. These words increase the probability of a
type I error, where the null hypothesis is rejected even if the word embedding is not
biased.

\subsubsection{Frequency weighting method} \label{section:discussion_frequency_method}
This approach builds upon the assumption that words that are regarded as neutral words
(e.g. \textit{and}, \textit{the}) should be gender neutral, unless there exists some form
of gender bias in the word embedding.
The advantage of this second approach is that results are more representative of how
the language is actually used. While the amount of words associated with men and the
amount of words associated with women might be equal, i.e. each male word has a female
counterpart, they are not used as often.
Taking the frequency of words into account addresses this issue. 
The disadvantage of this is that
it might miss forms of gender bias that might be present in the word embedding, because
not all words are considered to be as important as other words. For example, if the words
\textit{doctor} and \textit{nurse} have a large amount of bias associated with them,
but they only have a weight of $0.00001$, then their reported bias would be almost
invisible when compared to a presumably neutral word like \textit{and}, which might have
a weight of $0.1$. Words that are not used as often will therefore have a negligible impact
on the measurement.






There are some limitations with the used method, mainly that it is built upon quite a few
assumptions.
The assumption that the attribute words `male' and `female' are universal is difficult
to verify without having a good understanding of all the languages that are involved.
This issue can be solved by only looking at the languages that I have a thorough
understanding of, but this would not enable me to look at languages that are completely
different from English. This limitation can also be alleviated by using more than one
attribute word for each gender, as done by \textcite{caliskan_2017_semantics_language_corpora}.

Another assumption is that the most commonly used words, like \textit{and} and \textit{the}
are gender neutral. This might not necessarily the case, since words like \textit{he} and
\textit{she} are also very common. Attaching a weight based on the frequency might do
nothing more than measure the frequency of gender specific words, because they have a
more pronounced cosine distance. If the word \textit{he} occurs much more often than
the word \textit{she}, then the language will appear to have a strong bias towards
\textit{male}. While one could argue that the fact that masculine words have a higher
frequency is also a form of bias, it is probably not something that has an impact when
these word embeddings are actually used. If an application cares about frequency of words,
it will probably use a different tool.

It is also difficult to draw any conclusions about the differences between the languages
themselves, because it is not fair to assume that a word embedding is a perfect
representation of a language. The used models have been trained on text that was 
posted on the internet~\parencite{grave2018learning}. The writers of these texts might
not fully represent the speakers of the language.
Besides that, it is difficult to assume that this is
good method of measuring gender bias of a language as a whole. The WEAT method by
\textcite{caliskan_2017_semantics_language_corpora} has received critique from
\textcite{ethayarajh-etal-2019-understanding}, who argue that WEAT has theoretical flaws 
that cause it to systematically over-estimate bias.
